{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8669d6d4-06ce-475c-99be-e1166bf3e1be",
   "metadata": {},
   "source": [
    "# K-nearest Neighbors Regression — Warmup Activity\n",
    "\n",
    "This warm-up activity demonstrates how a the k-nearest neighbors regression algorithm predicts the value at unknown points.\n",
    "\n",
    "We begin, as usual, by importing the python libraries we need and setting a few matplotlib plotting options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae3266f-6cfb-4f46-b36c-caef6d65b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datascience import Table\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot defaults\n",
    "plt.rcParams['figure.figsize'] = (5, 5)\n",
    "plt.rcParams['axes.grid'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbab42b-84b2-475e-b5a5-3328016027ce",
   "metadata": {},
   "source": [
    "## Create a small data set with three clusters of points. \n",
    "Each cluster is comprised of points assigned randomly around different center points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29daafa3-7301-43b1-ba8c-1343d85dcbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rng stands for random number generator\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# --- Make a 2-D feature dataset with a smooth target y + noise ---\n",
    "# Think of (x1, x2) as two measurable traits; y is a quantity we want to predict.\n",
    "n = 150\n",
    "x1 = rng.uniform(-3, 3, size=n)\n",
    "x2 = rng.uniform(-3, 3, size=n)\n",
    "\n",
    "# Smooth underlying function + noise\n",
    "y_true = np.sin(x1) + 0.5 * x2\n",
    "y = y_true + rng.normal(0, 0.35, size=n)\n",
    "\n",
    "data = Table().with_columns(\n",
    "    \"x1\", x1,\n",
    "    \"x2\", x2,\n",
    "    \"y\",  y\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1112e61e-ca36-4db5-bfc5-5a96dc63f904",
   "metadata": {},
   "source": [
    "## Data characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912f635-2488-4d1d-9f29-8f15f2956e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e65ea9-6c0d-4409-bdbd-23243d1122a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4c787f-0237-4fd4-ab7c-c9fea1e65010",
   "metadata": {},
   "source": [
    "## Student Challenge #1\n",
    "What is the standard deviation of the data? (add code cells as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd04ee00-5956-434c-ab65-6351d26e5211",
   "metadata": {},
   "source": [
    "## Split the data\n",
    "In Lab 10 we learn that:\n",
    "\"A key concept in machine learning is using a subset of a dataset to train an algorithm to make estimates on a separate set of test data. The quality of the machine learning and algorithm can be assesed based on the accuracy of the predictions made on test data. Many times there are also parameters sometimes termed hyper-parameters which can be optimized through an iterative approach on test or validation data. In practice a dataset is randomly split into training and test sets using sampling.\"\n",
    "\n",
    "In this case we will use 70% of the data to train the model and then try to predict the other 30%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022fc3eb-437f-41ce-9008-06939ea1b745",
   "metadata": {},
   "source": [
    "## Student Challenge 1\n",
    "Explain the trade-offs when deciding what fraction of the data to use for training and what fraction for testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1d9bc-ef79-4a17-a36f-8864c15865dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f060cd-f98f-4749-872c-a9a01c4fc8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split (simple holdout)\n",
    "perm = rng.permutation(n)\n",
    "split = int(n * 0.7)\n",
    "train_idx, test_idx = perm[:split], perm[split:]\n",
    "\n",
    "train = data.take(train_idx)\n",
    "test  = data.take(test_idx)\n",
    "\n",
    "print(f\"Train: {train.num_rows}, Test: {test.num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d8f806-e7fb-4a22-82de-5fcfa251ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training targets (color by y)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plt.scatter(train.column(\"x1\"), train.column(\"x2\"),\n",
    "            c=train.column(\"y\"), s=30, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"y (target)\")\n",
    "plt.title(\"Training data: color = target y\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "\n",
    "# --- Make it square (1:1 aspect ratio) ---\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f6d3a-8a22-4559-a62f-84d20fb16c9b",
   "metadata": {},
   "source": [
    "The values of y are color-coded. We see the data does not follow any simple linear pattern. Note, however, that higher values of y tend to be located in the upper right quandrant, associated with higher values of x1 and x2, while lower values of y are located in in the lower left quandrant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b978cead-ce0d-48bd-985d-60f5bff367fc",
   "metadata": {},
   "source": [
    "## Making a prediction using the nearest neighbors\n",
    "Now suppose we wanted to predict the value of y at a point where we have no data. For example, at\n",
    "\n",
    "x1=0.5, x2=0.5\n",
    "\n",
    "What would we do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a27c56-c500-4ecd-962c-03fa99b23ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training targets (color by y)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plt.scatter(train.column(\"x1\"), train.column(\"x2\"),\n",
    "            c=train.column(\"y\"), s=30, cmap=\"viridis\")\n",
    "\n",
    "# --- Target point (the one we’ll predict) ---\n",
    "target = np.array([0.5, 0.5])\n",
    "plt.scatter(target[0], target[1], c='black', s=120, marker='X', label='Target point')\n",
    "plt.colorbar(label=\"y (target)\")\n",
    "plt.title(\"Training data: X marks the target y\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "\n",
    "# --- Make it square (1:1 aspect ratio) ---\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5bbfcd-7b35-451f-a58a-d6ed5f58922e",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "The functions below are the heart of the k-nearest neighbor regression method.\n",
    "We need functions to:\n",
    "\n",
    "* Find the distance from the unknown point to all of the known points\n",
    "* Return the closest k points\n",
    "* Predict the value at an unknown point by averaging the k closest neighbors\n",
    "* Calculate the error of our predictions by using the training data to predict the test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab34ac-bacf-4ea3-9bf7-6e7b8dee6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(p, q):\n",
    "    \"\"\"Euclidean distance between two 2-D points p and q.\"\"\"\n",
    "    dx = p[0] - q[0]\n",
    "    dy = p[1] - q[1]\n",
    "    return (dx*dx + dy*dy)**0.5\n",
    "\n",
    "def knn_indices(point, X_train, k):\n",
    "    \"\"\"\n",
    "    Return indices of the k nearest neighbors in X_train to 'point'.\n",
    "    \"\"\"\n",
    "    dists = []\n",
    "    for i, p in enumerate(X_train):\n",
    "        dists.append((distance(point, p), i))\n",
    "    dists.sort(key=lambda t: t[0])     # sort by distance\n",
    "    idxs = [i for (_, i) in dists[:k]]\n",
    "    return idxs\n",
    "\n",
    "def predict_knn(point, X_train, y_train, k):\n",
    "    \"\"\"\n",
    "    Predict numeric target for a single point = average of the k nearest y's.\n",
    "    \"\"\"\n",
    "    idxs = knn_indices(point, X_train, k)\n",
    "    return float(np.mean(y_train[idxs]))\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Find the root mean square error of the predictions.\n",
    "    \"\"\"\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df31e395-e916-4765-a66b-1ecca962d7b9",
   "metadata": {},
   "source": [
    "So to find the value at x1=0.5, x2=0.5,\n",
    "if we were using the five nearest neighbors (k=5), it would look like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f446fc01-0c8f-4cbc-87e4-7456007540d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your existing training Table to arrays\n",
    "X_train = np.column_stack([train.column(\"x1\"), train.column(\"x2\")])\n",
    "y_train = train.column(\"y\")\n",
    "\n",
    "# Convert Tables to NumPy arrays for mathEolumn(\"x1\"),  test.column(\"x2\")])\n",
    "y_test  = test.column(\"y\")\n",
    "\n",
    "# create the target point (x1, x2)\n",
    "target = np.array([0.5, 0.5])\n",
    "\n",
    "# Number of neighbors\n",
    "k = 5\n",
    "\n",
    "# Compute distances from all training points\n",
    "dists = np.array([distance(p, target) for p in X_train])\n",
    "\n",
    "# Get indices of k nearest neighbors\n",
    "nearest_idx = np.argsort(dists)[:k]\n",
    "r = dists[nearest_idx[-1]]  # distance to kth neighbor (for circle)\n",
    "\n",
    "# --- Plot ---\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sc = plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', s=40, label='Training data')\n",
    "plt.colorbar(sc, label='Target value (y)')\n",
    "\n",
    "# Highlight the k nearest neighbors\n",
    "plt.scatter(X_train[nearest_idx, 0], X_train[nearest_idx, 1],\n",
    "            edgecolor='red', facecolor='none', s=120, linewidth=2, label=f'{k} nearest neighbors')\n",
    "\n",
    "# Plot the target point itself\n",
    "plt.scatter(target[0], target[1], c='black', s=150, marker='X', label='Target point')\n",
    "\n",
    "# Circle showing the distance to the kth neighbor\n",
    "circle = plt.Circle(target, r, color='red', fill=False, linestyle='--', linewidth=1.8)\n",
    "plt.gca().add_artist(circle)\n",
    "\n",
    "plt.xlabel('x1'); plt.ylabel('x2')\n",
    "plt.title(f'KNN Visualization (k={k})\\nTarget x1={target[0]}, x2={target[1]} and its nearest neighbors')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Make it square (1:1 aspect ratio) ---\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print out neighbor coordinates and distances\n",
    "neighbor_info = np.column_stack([X_train[nearest_idx], y_train[nearest_idx], dists[nearest_idx]])\n",
    "print(\"Nearest neighbors (x1, x2, y, distance):\\n\", np.round(neighbor_info, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411ecf5-1a3d-43c2-a634-400a47d0d1a2",
   "metadata": {},
   "source": [
    "## Student Challenge 2\n",
    "In plain words, what does KNN regression do to make a prediction? (Write a paragraph or more.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb275a4-dfd3-442e-80db-511d94b08ead",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4dd6176-435a-48d5-9bcb-2a74b8c541b5",
   "metadata": {},
   "source": [
    "## What's the best choice k, the number of neighbors?\n",
    "As mentioned above in Lab 10:\n",
    "\"Many times there are also parameters sometimes termed hyper-parameters which can be optimized through an iterative approach on test or validation data.\"\n",
    "\n",
    "For k-nearest neighbors regression there is one hyperparameter -- k.\n",
    "\n",
    "The optimal choice of the number of neighbors to use in k-means regression varies depending on the data set. The way to find the best choice to try different k-values and look at the RMSE for the training data. \n",
    "\n",
    "Let's try a variety of k's and see which works best for our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e8d2ca-ab48-4cd9-a034-ec1a0158cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curve: RMSE vs k (fixed)\n",
    "\n",
    "# Convert training and test Tables to NumPy arrays for features and targets\n",
    "X_train = np.column_stack([train.column(\"x1\"), train.column(\"x2\")])\n",
    "y_train = np.array(train.column(\"y\"))\n",
    "\n",
    "X_test  = np.column_stack([test.column(\"x1\"),  test.column(\"x2\")])\n",
    "y_test  = np.array(test.column(\"y\"))\n",
    "\n",
    "# Try a range of k values and compute RMSE on the test set\n",
    "Ks = [1, 3, 5, 7, 9, 11, 15, 21, 31]\n",
    "rmses = []\n",
    "\n",
    "for K in Ks:\n",
    "    y_pred_K = np.array([predict_knn(pt, X_train, y_train, K) for pt in X_test])\n",
    "    rmses.append(rmse(y_test, y_pred_K))\n",
    "\n",
    "plt.plot(Ks, rmses, marker=\"o\")\n",
    "plt.title(\"Validation curve: RMSE vs k\")\n",
    "plt.xlabel(\"k (neighbors)\")\n",
    "plt.ylabel(\"Test RMSE (lower is better)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f79976-50ab-44d1-8e07-91f977121524",
   "metadata": {},
   "source": [
    "## Student Challenge 3:\n",
    "\n",
    "What is the best choice of k for this data set?  How much do you think it matters if you choose a slightly suboptimal value of k?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7dcd98-a96e-458a-bcc1-debf01a10536",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20ae2de3-2334-44fe-923c-9be2583e2ecb",
   "metadata": {},
   "source": [
    "## Looking at the full prediction space\n",
    "To get a full sense of the k-nearest neighbors regression predictions, we can use the algorithm to predict y values over a dense grid of point and contour the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb9ad9-d764-4d20-a75b-21f1e9b68745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a grid over the feature space\n",
    "g = 50\n",
    "gx = np.linspace(-3, 3, g)\n",
    "gy = np.linspace(-3, 3, g)\n",
    "GX, GY = np.meshgrid(gx, gy)\n",
    "grid_pts = np.column_stack([GX.ravel(), GY.ravel()])\n",
    "\n",
    "# Predict on grid\n",
    "k_map = 9  # pick a moderate k for a smooth map\n",
    "grid_pred = np.array([predict_knn(pt, X_train, y_train, k_map) for pt in grid_pts]).reshape(g, g)\n",
    "\n",
    "# Plot the prediction field + training points\n",
    "plt.figure(figsize=(6,5))\n",
    "cn = plt.contourf(GX, GY, grid_pred, levels=20, cmap=\"viridis\")\n",
    "plt.colorbar(cn, label=\"Predicted ŷ\")\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=\"white\", s=10, alpha=0.6, label=\"train pts\")\n",
    "plt.title(f\"KNN Regression Prediction Map (k={k_map})\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb857f5-a2ca-4a0f-a7a1-727e9dd06e43",
   "metadata": {},
   "source": [
    "## Student Challenge 4\n",
    "Based on the figure above, roughly what would be the predicted value at x1=1.0, x2=0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbf8175-eb43-48ba-8080-c06886cd1505",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9b46363-d82f-433c-aed0-70db3c9f70dd",
   "metadata": {},
   "source": [
    "## Student Challenge 5\n",
    "In this exercise, we use two features, x1 and x2, to predict the value of our target, y. Do you think this method could be generalized to predict a target value from more than two features? How would that work?\n",
    "(Write a paragraph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f0de6-1a59-4a27-b968-a4cf692dc636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
